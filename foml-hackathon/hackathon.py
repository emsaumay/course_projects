# -*- coding: utf-8 -*-
"""cs24mtech14005_foml24_hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ne6wb7ErQw-8gfn3Ttn7kRbBSY9OgpQs
"""

# Import the pandas library for data manipulation and analysis
import pandas as pd

# Import the NumPy library for numerical operations
import numpy as np

# Import the Seaborn library for statistical data visualization
import seaborn as sns

# Import Matplotlib for plotting data
import matplotlib.pyplot as plt

# Import KNNImputer for imputing missing values using K-Nearest Neighbors
from sklearn.impute import KNNImputer

# Import LabelEncoder to convert categorical labels into numerical format
from sklearn.preprocessing import LabelEncoder

# Import the RandomForestClassifier for feature selectiontask
from sklearn.ensemble import RandomForestClassifier

# Import the BalancedRandomForestClassifier for imbalanced classification tasks
from imblearn.ensemble import BalancedRandomForestClassifier

# Import warnings module to handle warning messages
import warnings
warnings.filterwarnings('ignore')

# Set minimum rows and maximum columns displayed when printing a DataFrame to 200
pd.options.display.min_rows = 200
pd.options.display.max_columns = 200

def analyze_train_data(train_data):
    print("\n Analyzing Data:\n")
    # Printing Null Values % in features
    print(train_data.isnull().sum()/train_data.shape[0])

    # Plotting correlation
    for feature in ['AgriculturalPostalZone', 'AgricultureZoningCode', 'CropSpeciesVariety', 'DistrictId', 'FieldEstablishedYear', 'LandUsageType', 'MainIrrigationSystemCount', 'RawLocationId', 'TaxAgrarianValue', 'TaxLandValue', 'TotalTaxAssessed', 'TotalValue', 'TownId', 'ValuationYear', 'WaterAccessPoints']:
        data_filtered = train_data[['Longitude', 'Latitude', feature]]

        # Identify rows where feature is null
        null_values = data_filtered[feature].isnull()

        # Plot the relationship
        plt.figure(figsize=(30, 25))

        # Plot rows where feature is not null with the colormap
        scatter = plt.scatter(
            data_filtered.loc[~null_values, 'Longitude'],
            data_filtered.loc[~null_values, 'Latitude'],
            c=data_filtered.loc[~null_values, feature],
            cmap='viridis',
            alpha=0.7,
            label=f'{feature} Present'
        )

        # Plot rows where feature is null in red
        plt.scatter(
            data_filtered.loc[null_values, 'Longitude'],
            data_filtered.loc[null_values, 'Latitude'],
            color='red',
            alpha=0.5,
            label=f'{feature} Missing'
        )

        # Add colorbar for the non-null feature values
        plt.colorbar(scatter, label=feature)

        # Set labels and title
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.title(f'Relationship between Longitude, Latitude, and {feature}')
        plt.grid(visible=False, linestyle='--', alpha=0.5)
        plt.legend()
        plt.show()


    # Plotting correlation
    correlation_matrix = train_data.corr()

    # Create the heatmap
    plt.figure(figsize=(42, 27))
    sns.heatmap(correlation_matrix, annot=True)
    plt.title('Correlation Matrix of Features')
    plt.show()

def labeling_feature(data, label_encoder=None):
    if label_encoder == None:
        # print("\nLabeling Features:")
        label_encoder = LabelEncoder()

        # Fit the label encoder on the 'Target' column in the data and transform it into numerical labels
        data['Target'] = label_encoder.fit_transform(data['Target'])

        # Create a mapping between the original labels and the encoded labels
        label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
        # print(label_mapping)
        return data, label_encoder
    else:
        # print("\nInverse Transforming Features:")
        # Apply the inverse transformation using the provided label_encoder
        data = label_encoder.inverse_transform(data)
        return data

def drop_null_features(train_data, test_data, threshold=0.9):

    # Calculate the percentage of null values
    null_percentage = train_data.isnull().sum() / train_data.shape[0]

    # Identify columns to drop based on the threshold for missing values
    columns_to_drop = null_percentage[null_percentage > threshold].index

    # Drop the identified columns from both training and test data
    train_data = train_data.drop(columns=columns_to_drop, axis=1)
    test_data = test_data.drop(columns=columns_to_drop, axis=1)

    # print(f"Columns dropped: {columns_to_drop}")
    return train_data, test_data

def drop_null_rows(train_data, threshold=0.02):
    # print("\nDropping Null Rows:")
    threshold = threshold * train_data.shape[0]
    # print(f"Rows dropped from: ")

    # Iterate through each column to check the number of missing values
    for column in train_data.columns:
        na_count = train_data[column].isna().sum()

        # If the missing value count is less than the threshold, keep the rows that are not NaN
        if na_count < threshold:
            rows_to_keep = ~train_data[column].isna()
            train_data = train_data[rows_to_keep]

            # print(f"{column}")
    return train_data

def impute_using_KNNImputer(train_data, test_data, features_to_impute):
    # print(f"\nImputing Missing Values: {features_to_impute}")

    # Loop through each feature that needs missing values imputed
    for feature in features_to_impute:
        raw_data = train_data[['Longitude', 'Latitude', feature]]
        raw_data_filtered = raw_data.dropna(subset=[feature])
        imputer = KNNImputer()
        imputer.fit(raw_data_filtered)

        # Impute missing values for the feature across the entire dataset
        imputed_data = imputer.transform(raw_data)
        train_data[feature] = imputed_data[:, 2]

        # Impute missing values for the feature in the test data
        imputed_test_data = imputer.transform(test_data[['Longitude', 'Latitude', feature]])
        test_data[feature] = imputed_test_data[:, 2]

        # Ploting Training data Graph
        # plt.figure(figsize=(30, 25))

        # scatter = plt.scatter(
        #     train_data['Longitude'],
        #     train_data['Latitude'],
        #     c=train_data[feature],
        #     cmap='viridis',
        #     alpha=0.7,
        #     label=f'{feature} Present'
        # )

        # # Add colorbar for the feature values
        # plt.colorbar(scatter, label=feature)

        # # Set labels and title
        # plt.xlabel('Longitude')
        # plt.ylabel('Latitude')
        # plt.title(f'Relationship between Longitude, Latitude, and {feature}')
        # plt.grid(visible=False, linestyle='--', alpha=0.5)
        # plt.legend()
        # plt.show()
    return train_data, test_data

def impute_using_mean(train_data, test_data, feature_to_impute):
    # print(f"\nImputing Missing Values using mean: {feature_to_impute}")

    # Calculate the mean of the feature in the training dataset
    feature_mean = train_data[feature_to_impute].mean()

    # Fill missing values in the training dataset with the calculated mean
    train_data[feature_to_impute] = train_data[feature_to_impute].fillna(feature_mean)

    # Apply the same mean to fill missing values in the test dataset
    test_data[feature_to_impute] = test_data[feature_to_impute].fillna(feature_mean)
    return train_data, test_data

def impute_using_mode(train_data, test_data, feature_to_impute):
    # print(f"\nImputing Missing Values using mode: {feature_to_impute}")

    # Calculate the mode (most frequent value) of the feature in the training dataset
    feature_mode = train_data[feature_to_impute].mode()[0]

    # Fill missing values in the training dataset with the calculated mode
    train_data[feature_to_impute] = train_data[feature_to_impute].fillna(feature_mode)

    # Apply the same mode to fill missing values in the test dataset
    test_data[feature_to_impute] = test_data[feature_to_impute].fillna(feature_mode)
    return train_data, test_data

def impute_using_groupby(train_data, test_data, feature_to_impute, grouping_features):
    # print(f"\nImputing Missing Values using GroupBy: {feature_to_impute}")

    # Impute missing values in the training dataset using the group-by mean for each grouping feature
    for grouping_feature in grouping_features:
        train_data[feature_to_impute] = train_data[feature_to_impute].fillna(
            train_data.groupby(grouping_feature)[feature_to_impute].transform('mean')
        )
    # If there are still missing values, impute them using the overall mean of the feature in training data
    train_data[feature_to_impute] = train_data[feature_to_impute].fillna(train_data[feature_to_impute].mean())

    # Impute missing values in the test dataset using the same group-by means from the training data
    for grouping_feature in grouping_features:
        test_data[feature_to_impute] = test_data[feature_to_impute].fillna(
            test_data[grouping_feature].map(train_data.groupby(grouping_feature)[feature_to_impute].transform('mean'))
        )
    # If there are still missing values, impute them using the overall mean of the feature in training data
    test_data[feature_to_impute] = test_data[feature_to_impute].fillna(train_data[feature_to_impute].mean())
    return train_data, test_data

def data_preprocessing(train_data, test_data):
    # Drop columns with more than 90% missing values in both train and test data
    train_data, test_data = drop_null_features(train_data, test_data, threshold=0.9)

    # Drop rows with more than 2% missing values in the training data
    train_data = drop_null_rows(train_data, threshold=0.02)

    # Impute missing values using KNN Imputer for specified features
    features_to_impute = ["HarvestProcessingType", "TypeOfIrrigationSystem"]
    train_data, test_data = impute_using_KNNImputer(train_data, test_data, features_to_impute)

    # Impute missing values using group-by means for 'CultivatedAreaSqft1' and other features
    train_data, test_data = impute_using_groupby(train_data, test_data, "CultivatedAreaSqft1",
                                                 ["TotalCultivatedAreaSqft", "MainIrrigationSystemCount", "TaxAgrarianValue"])
    # Impute missing values for 'FieldSizeSqft' using 'TownId' as a grouping feature
    train_data, test_data = impute_using_groupby(train_data, test_data, "FieldSizeSqft", ["TownId"])

    # Impute missing values for 'SoilFertilityType' using the mode (most frequent value)
    train_data, test_data = impute_using_mode(train_data, test_data, "SoilFertilityType")

    # Impute missing values using the mean for any remaining columns with missing values in the train data
    for col in train_data.columns[train_data.isnull().any()]:
        train_data, test_data = impute_using_mean(train_data, test_data, col)

    # Impute missing values using the mean for any remaining columns with missing values in the test data
    for col in test_data.columns[test_data.isnull().any()]:
        train_data, test_data = impute_using_mean(train_data, test_data, col)

    return train_data, test_data

def feature_engineering(data):
    # print("\nFeature Engineering..")

    # Create a new feature: TaxBurdenRatio (Tax assessment vs. total value)
    data['TaxBurdenRatio'] = data['TotalTaxAssessed'] / (data['TotalValue'] + 1)

    # Create a new feature: AgrarianValueRatio (Agrarian value vs. total value)
    data['AgrarianValueRatio'] = data['TaxAgrarianValue'] / (data['TotalValue'] + 1)

    # Calculate Farm Age based on the difference between the latest valuation year and the field establishment year
    data['FarmAge'] = data['ValuationYear'].max() - data['FieldEstablishedYear']

    # IrrigationCount (Main and partial irrigation systems with weighted count for partial)
    data["IrrigationCount"] = (data["MainIrrigationSystemCount"] +
                               (data["PartialIrrigationSystemCount"].fillna(0) * 0.5))

    # IrrigationDensity (Irrigation count per unit of cultivated area)
    data["IrrigationDensity"] = (data["IrrigationCount"] / data["TotalCultivatedAreaSqft"])

    # WaterResourceAvailability (sum of water access points, reservoirs, and weighted lake presence)
    data["WaterResouceAvailability"] = (data["WaterAccessPoints"] + data["WaterReservoirCount"] +
                                        (data["NaturalLakePresence"] * 0.5) *
                                        (data["WaterAccessPoints"] + data["WaterReservoirCount"]))

    # WaterDensity (Water resource availability per unit of cultivated area)
    data["WaterDensity"] = (data["WaterResouceAvailability"] / data["TotalCultivatedAreaSqft"])

    # FieldEfficiency (based on soil fertility, water availability, farm vehicles, and irrigation)
    data["FieldEfficiency"] = (data["SoilFertilityType"] + data["WaterResouceAvailability"] +
                               data["FarmVehicleCount"] + data["IrrigationCount"] +
                               data["StorageAndFacilityCount"]) / data["FieldSizeSqft"]
    return data

def drop_features_with_most_uniques(train_data, test_data, features_to_drop):
    # print(f"\nDropping Features with Most Unique Values: {features_to_drop}")

    # Drop the specified features from the training dataset
    train_data = train_data.drop(columns=features_to_drop, axis=1)

    # Drop the specified features from the test dataset
    test_data = test_data.drop(columns=features_to_drop, axis=1)
    return train_data, test_data

def feature_selection(X_train, y_train, test_data):

    # Random Forest Classifier model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Get the feature importances from the trained model
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    # Select the top 40 most important features
    top_40_features = indices[:40]
    train_data_selected = X_train.iloc[:, top_40_features]
    test_data_selected = test_data.iloc[:, top_40_features]

    return train_data_selected, test_data_selected

def train_and_evaluate_model(X_train, y_train, test_data):
    # Initialize the BalancedRandomForestClassifier with specified hyperparameters
    model = BalancedRandomForestClassifier(
        min_samples_split=5,
        max_depth=20,
        n_estimators=300,
        min_samples_leaf=5,
        random_state=42
    )

    # Train the model on the training data (X_train, y_train)
    model.fit(X_train, y_train)

    # Predict the target values for the test dataset
    prediction = model.predict(test_data)
    return prediction

import argparse
def make_predictions(test_fname, predictions_fname):
    # Load the training data from the provided URL
    train_file_path = "train.csv"
    train_data = pd.read_csv(train_file_path)
    train_data, label_encoder = labeling_feature(train_data)

    # Analyze and preprocess the training data (check for missing values, distributions, etc.)
    # analyze_train_data(train_data)

    # Load the test data
    test_data = pd.read_csv(test_fname)

    # Preprocess the data
    train_data, test_data = data_preprocessing(train_data, test_data)

    # Apply feature engineering to create new features
    train_data = feature_engineering(train_data)
    test_data = feature_engineering(test_data)

    # Drop features with too many unique values (non-informative or redundant columns)
    train_data, test_data = drop_features_with_most_uniques(train_data, test_data, ["UID", "Latitude", "Longitude"])

    # Split the data into features (X) and target (y) for model training
    X_train = train_data.drop('Target', axis=1)
    y_train = train_data['Target']

    # Select the most important features based on model performance
    X_train, test_data = feature_selection(X_train, y_train, test_data)

    # Train the model and get predictions for the test set
    prediction_raw = train_and_evaluate_model(X_train, y_train, test_data)
    prediction = labeling_feature(prediction_raw, label_encoder)

    # Save the predictions to a CSV file with 'UID' and 'Target'
    test_original_data = pd.read_csv(test_fname)
    submission_data = pd.DataFrame({'UID': test_original_data['UID'], 'Target': prediction})

    # Write the submission data to a CSV file
    submission_data.to_csv(predictions_fname, index=False)


if __name__=="__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-file", type=str, help='file path of train.csv')
    parser.add_argument("--test-file", type=str, help='file path of test.csv')
    parser.add_argument("--predictions-file", type=str, help='save path of predictions')
    args = parser.parse_args()
    make_predictions(args.test_file, args.predictions_file)